import streamlit as st
import requests
import json
import os
import time
import whisper
from dotenv import load_dotenv

# ===================== 1. åŸºç¡€é…ç½®ä¸è§†è§‰é£æ ¼æ³¨å…¥ =====================
load_dotenv()
st.set_page_config(
    page_title="é£ä¹¦çº§æ™ºèƒ½çºªè¦-æè‡´è¿˜åŸç‰ˆ",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ ¸å¿ƒå¯†é’¥é…ç½®
QWEN_API_KEY = "sk-ecb46034c430477e9c9a4b4fd6589742"
FEISHU_WEBHOOK = st.secrets.get("FEISHU_WEBHOOK", "")

# æ³¨å…¥ CSSï¼šå®Œç¾å¤åˆ»é£ä¹¦å¡ç‰‡å®¹å™¨ã€è‰²å—æ ‡ç­¾å’Œæ’ç‰ˆ
st.markdown("""
<style>
    .feishu-box {
        background-color: #ffffff;
        border: 1px solid #dee0e3;
        border-radius: 10px;
        padding: 24px;
        box-shadow: 0 4px 12px rgba(31,35,41,0.08);
        margin-bottom: 20px;
        font-family: "PingFang SC", "Microsoft YaHei", sans-serif;
    }
    .section-title { font-size: 18px; font-weight: 600; color: #1f2329; margin: 20px 0 16px 0; border-bottom: 1px solid #f2f3f5; padding-bottom: 8px; }
    .tag { padding: 2px 8px; border-radius: 4px; font-size: 12px; font-weight: bold; margin-left: 8px; vertical-align: middle; }
    .tag-green { background: #e8f8f2; color: #00b67a; } /* æ­£å¸¸æ¨è¿› / å·²å®Œæˆ [cite: 10, 31] */
    .tag-orange { background: #fff7e8; color: #ff9d00; } /* éœ€è¦ä¼˜åŒ– [cite: 12] */
    .tag-red { background: #fff2f0; color: #f53f3f; } /* å­˜åœ¨é£é™© [cite: 14] */
    .tag-blue { background: #e8f3ff; color: #165dff; } /* è¿›è¡Œä¸­ */
    .next-plan-box { background-color: #fff7e8; border-radius: 4px; padding: 12px; border-left: 4px solid #ff9d00; margin: 15px 0; }
</style>
""", unsafe_allow_html=True)

# ===================== 2. æ ¸å¿ƒè½¬å½•ä¸å‡€åŒ–é€»è¾‘ =====================

@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base") # å¹³è¡¡é€Ÿåº¦ä¸ç²¾åº¦

whisper_model = load_whisper_model()

def audio_to_text(audio_file):
    """
    æœ¬åœ°Whisperè½¬å†™ï¼šä¼˜åŒ–å‘è¨€äººåŒºåˆ†ï¼ˆ3ç§’åœé¡¿ï¼‰+æœ¯è¯­ç²¾å‡†è¯†åˆ«+æ ¼å¼ä¿®æ­£
    """
    temp_audio_path = f"temp_{audio_file.name}"
    with open(temp_audio_path, "wb") as f:
        f.write(audio_file.getbuffer())
    
    result = whisper_model.transcribe(
        temp_audio_path,
        language="zh",
        word_timestamps=True,
        fp16=False
    )
    
    transcript = []
    speaker_id = 1
    last_end_time = 0
    filler_words = ["å—¯", "å•Š", "è¿™ä¸ª", "é‚£ä¸ª", "ç„¶å", "å…¶å®", "å°±æ˜¯è¯´", "å¥½çš„", "è¡Œ", "å“¦", "å‘ƒ", "å¯¹"]
    key_terms = ["æ–‡ä»¶æŸœ", "é¤è¾¹æŸœ", "æ–—æŸœ", "é‹æŸœ", "é¢†æ˜Ÿç³»ç»Ÿ", "äº‘ä»“", "CGè´¦å·", "ROAS", "UPC", "SKU"]
    
    for segment in result["segments"]:
        # åˆ¤å®šå‘è¨€äººåˆ‡æ¢ï¼šåœé¡¿ â‰¥ 3ç§’ [cite: 37, 49]
        if segment["start"] - last_end_time >= 3 and len(transcript) > 0:
            speaker_id += 1
        last_end_time = segment["end"]
        
        clean_text = segment["text"]
        for word in filler_words: clean_text = clean_text.replace(word, "")
        for term in key_terms:
            if term.lower() in clean_text.lower(): clean_text = clean_text.replace(term.lower(), term)
        
        # æ•°å­—æ ¼å¼ä¿®æ­£
        for i, cn_num in enumerate(["ä¸€","äºŒ","ä¸‰","å››","äº”","å…­","ä¸ƒ","å…«","ä¹","å"]):
            clean_text = clean_text.replace(cn_num, str(i+1))
            
        if clean_text.strip():
            transcript.append({
                "speaker": f"å‘è¨€äºº{speaker_id}",
                "text": clean_text.strip(),
                "time": f"{int(segment['start']//60):02d}:{int(segment['start']%60):02d}"
            })
    
    os.remove(temp_audio_path)
    return transcript

# ===================== 3. é£ä¹¦æ ¼å¼åŒ–ä¸ AI ç”Ÿæˆé€»è¾‘ =====================

def fix_feishu_visuals(text):
    """
    1:1 å¤åˆ»é£ä¹¦æ’ç‰ˆæ ·å¼ä¸è‰²å—æ ‡ç­¾
    """
    # è½¬æ¢çŠ¶æ€æ ‡ç­¾
    text = text.replace("[æ­£å¸¸æ¨è¿›]", '<span class="tag tag-green">æ­£å¸¸æ¨è¿›</span>')
    text = text.replace("[å·²å®Œæˆ]", '<span class="tag tag-green">å·²å®Œæˆ</span>')
    text = text.replace("[éœ€è¦ä¼˜åŒ–]", '<span class="tag tag-orange">éœ€è¦ä¼˜åŒ–</span>')
    text = text.replace("[å­˜åœ¨é£é™©]", '<span class="tag tag-red">å­˜åœ¨é£é™©</span>')
    text = text.replace("[å¾…å¤„ç†]", '<span class="tag tag-red">å¾…å¤„ç†</span>')
    text = text.replace("[è®¡åˆ’ä¸­]", '<span class="tag tag-blue">è®¡åˆ’ä¸­</span>')
    
    # æ¨¡å—æ ‡é¢˜ HTML åŒ–
    text = text.replace("### æ€»ç»“", '<div class="section-title">ğŸ“Š é‡ç‚¹é¡¹ç›®æ¦‚è§ˆ</div>')
    text = text.replace("### è¿è¥å·¥ä½œè·Ÿè¿›", '<div class="section-title">ğŸ“… è¿è¥å·¥ä½œè·Ÿè¿›</div>')
    text = text.replace("### å…³é”®å†³ç­–", '<div class="section-title">ğŸ¯ å…³é”®å†³ç­–</div>')
    text = text.replace("### é‡‘å¥æ—¶åˆ»", '<div class="section-title">ğŸ’¬ é‡‘å¥æ—¶åˆ»</div>')
    text = text.replace("### å¾…åŠ", '<div class="section-title">âœ… å¾…åŠäº‹é¡¹</div>')
    
    # é‡‘å¥æ ·å¼
    text = text.replace("ã€Œ", "<i style='color:#646a73;'>ã€Œ").replace("ã€", "ã€</i>")
    
    return f'<div class="feishu-box">{text}</div>'

def generate_feishu_summary(transcript_data):
    """
    è°ƒç”¨é€šä¹‰åƒé—® Qwen-Maxï¼Œä¸¥æ ¼æ‰§è¡Œ 8 å¤§æ¨¡å— Prompt
    """
    url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
    headers = {"Authorization": f"Bearer {QWEN_API_KEY}", "Content-Type": "application/json"}
    
    prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„é£ä¹¦ï¼ˆLarkï¼‰æ™ºèƒ½çºªè¦åŠ©æ‰‹ï¼Œéœ€ä¸¥æ ¼æŒ‰ç…§é£ä¹¦æ™ºèƒ½çºªè¦æ ‡å‡†ç”Ÿæˆ 8 å¤§æ ¸å¿ƒæ¨¡å—ï¼Œè¿˜åŸåº¦100%ï¼š

ã€æ¨¡å—1ï¼šåŸºç¡€å…ƒä¿¡æ¯ã€‘å½•éŸ³ä¸»é¢˜ã€æ—¶é—´ï¼ˆXXXXå¹´XXæœˆXXæ—¥æ ¼å¼ï¼‰ã€AIç”Ÿæˆå…è´£å£°æ˜ã€‚
ã€æ¨¡å—2ï¼šæ ¸å¿ƒæ€»ç»“ã€‘åŒ…å«ã€Œæ€»ç»“ã€æ ‡é¢˜ã€ä¸€å¥è¯ä¸»é¢˜ã€é‡ç‚¹é¡¹ç›®ï¼ˆå¸¦ [æ­£å¸¸æ¨è¿›/éœ€è¦ä¼˜åŒ–/å­˜åœ¨é£é™©] çŠ¶æ€æ ‡ç­¾ï¼‰ã€‚[cite: 8, 10, 12, 14]
ã€æ¨¡å—3ï¼šè¿è¥å·¥ä½œè·Ÿè¿›ã€‘ç”¨å››åˆ—è¡¨æ ¼å±•ç¤ºï¼šå·¥ä½œç±»åˆ« | å…·ä½“å†…å®¹ | è´Ÿè´£äºº | çŠ¶æ€ã€‚[cite: 31]
ã€æ¨¡å—4ï¼šè¯¦ç»†ä¼šè®®å†…å®¹ã€‘â—¦ ç« èŠ‚ä¸»é¢˜ -> â–ª å­ä¸»é¢˜ï¼ŒæŒ‰é—®é¢˜+æ–¹æ¡ˆ+æ‰§è¡Œè¦æ±‚å±•å¼€ã€‚[cite: 35, 44]
ã€æ¨¡å—5ï¼šä¸‹ä¸€æ­¥è®¡åˆ’ã€‘ğŸ’¡ å¼€å¤´ï¼Œæ€»ç»“æ ¸å¿ƒåŠ¨ä½œï¼Œåˆ†ç‚¹åˆ—å‡ºæ¨¡å—ã€‚[cite: 32]
ã€æ¨¡å—6ï¼šå¾…åŠã€‘æ•°å­—ç¼–å·ï¼Œçº¯è¡ŒåŠ¨æŒ‡ä»¤ã€‚[cite: 98, 99]
ã€æ¨¡å—7ï¼šæ™ºèƒ½ç« èŠ‚ã€‘XX:XX ç« èŠ‚ä¸»é¢˜ + 100å­—ä»¥å†…æ¦‚æ‹¬ã€‚[cite: 104, 105]
ã€æ¨¡å—8ï¼šå…³é”®å†³ç­–+é‡‘å¥æ—¶åˆ»ã€‘é—®é¢˜/æ–¹æ¡ˆ/ä¾æ®é€»è¾‘ + è¯´è¯äººé‡‘å¥å¼•ç”¨ã€‚[cite: 127, 141]

ã€è½¬å†™åŸå§‹å†…å®¹ã€‘
{json.dumps(transcript_data, ensure_ascii=False)}
"""
    data = {
        "model": "qwen-max",
        "input": {"messages": [{"role": "user", "content": prompt}]},
        "parameters": {"result_format": "text", "temperature": 0.1, "max_output_tokens": 4096}
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=60)
        result = response.json()
        return result["output"]["text"]
    except Exception as e:
        st.error(f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        return None

def push_to_feishu_bot(summary):
    """
    å‘é€é£ä¹¦äº’åŠ¨å¡ç‰‡ï¼Œè¿˜åŸå¡ç‰‡è§†è§‰æ•ˆæœ
    """
    if not FEISHU_WEBHOOK: return
    # è½¬æ¢æ ‡ç­¾ä¸ºé£ä¹¦ Markdown è¡¨æƒ…
    card_text = summary.replace("[æ­£å¸¸æ¨è¿›]", "ğŸŸ¢ **æ­£å¸¸æ¨è¿›**").replace("[å­˜åœ¨é£é™©]", "ğŸ”´ **å­˜åœ¨é£é™©**")
    card_text = card_text.replace("[éœ€è¦ä¼˜åŒ–]", "ğŸŸ  **éœ€è¦ä¼˜åŒ–**").replace("[å·²å®Œæˆ]", "âœ… **å·²å®Œæˆ**")
    
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": "ğŸ“… é£ä¹¦æ™ºèƒ½ä¼šè®®çºªè¦"}, "template": "blue"},
            "elements": [
                {"tag": "div", "text": {"tag": "lark_md", "content": card_text}},
                {"tag": "hr"},
                {"tag": "note", "elements": [{"tag": "plain_text", "content": "ç”±é€šä¹‰åƒé—® Qwen-Max æè‡´è¿˜åŸç”Ÿæˆ"}]}
            ]
        }
    }
    requests.post(FEISHU_WEBHOOK, json=payload)

# ===================== 4. UI äº¤äº’å¸ƒå±€ =====================

st.title("ğŸ“ é£ä¹¦åŒæ¬¾æ™ºèƒ½çºªè¦ç”Ÿæˆå·¥å…·")
st.divider()

col_in, col_out = st.columns([1, 2], gap="large")

with col_in:
    st.subheader("ğŸ“¥ è¾“å…¥åŒºåŸŸ")
    audio_file = st.file_uploader("ä¸Šä¼ å½•éŸ³ (mp3/wav/m4a)", type=["mp3", "wav", "m4a"])
    st.markdown("---")
    text_input = st.text_area("æˆ–ç²˜è´´è½¬å†™æ–‡æœ¬", height=250, placeholder="å‘è¨€äºº1ï¼š...")
    generate_btn = st.button("ğŸš€ ç”Ÿæˆå¹¶å›ä¼ é£ä¹¦", type="primary", use_container_width=True)

with col_out:
    st.subheader("ğŸ“‹ é¢„è§ˆåŒºåŸŸ")
    result_area = st.empty()
    if generate_btn:
        with st.spinner("ğŸ§  æ­£åœ¨è¿›è¡Œå¤šç»´è¯­ä¹‰å¤åˆ»..."):
            # è·å–å†…å®¹
            if audio_file:
                transcript = audio_to_text(audio_file)
            elif text_input:
                transcript = [{"speaker": "å‘è¨€äºº1", "text": text_input, "time": "00:00"}]
            else:
                st.warning("è¯·è¾“å…¥å†…å®¹")
                st.stop()
            
            # ç”Ÿæˆçºªè¦
            raw_summary = generate_feishu_summary(transcript)
            if raw_summary:
                # ç½‘é¡µæ˜¾ç¤º HTML å¢å¼ºæ•ˆæœ
                formatted_html = fix_feishu_visuals(raw_summary)
                result_area.markdown(formatted_html, unsafe_allow_html=True)
                
                # é£ä¹¦æ¨é€
                push_to_feishu_bot(raw_summary)
                st.toast("âœ… çºªè¦å·²æ¨é€è‡³é£ä¹¦æœºå™¨äººï¼", icon="ğŸ“²")
